{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees and Phylogenetics\n",
    "In this week's notebook we will explore principals of phylogenetics and tree-like thinking using a combination of data analysis, empowered via biopython, and first principals coding work. \n",
    "\n",
    "### A simple tree representation\n",
    "One very simple and popular way to represent a tree is with what is known as a [Newick](https://en.wikipedia.org/wiki/Newick_format) format. A Newick tree uses a series of parentheses to describe the relations of taxa up a tree. To start I've included a toy tree example named `tree.newick` in the data directory. That file is written in the  which  In this case I've used the tree\n",
    "\n",
    "`(((1,2),(3,4)),(5,6));`\n",
    "\n",
    "Represented visually this simple tree looks like the following \n",
    "\n",
    "<img src=imgs/tree.png>\n",
    "\n",
    "Let's start by constructing trees using the UPGMA and Neighbor Joining algorithms that I introduced in class and then we will move to more industrial strength implementations using biopython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a upgma implementation\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def coal_label(str1, str2):\n",
    "    \"\"\"\n",
    "    utility function to build up newick tree string\n",
    "    \"\"\"\n",
    "    new = f\"({str1},{str2})\"\n",
    "    return(new)\n",
    "\n",
    "def find_min_idx(tmpMat):\n",
    "    \"\"\"\n",
    "    returns index of smallest cell assuming symmetric\n",
    "    matrix with distances on off-diagonal elements\n",
    "    \"\"\"\n",
    "    min = 1e6\n",
    "    min_i = min_j = -666\n",
    "    for i in range(tmpMat.shape[0]):\n",
    "        j = 0\n",
    "        while j < i:\n",
    "            if tmpMat[i,j] < min:\n",
    "                min_i = i\n",
    "                min_j = j\n",
    "                min = tmpMat[i,j]\n",
    "            j+=1\n",
    "        \n",
    "    return(min_i,min_j)\n",
    "\n",
    "\n",
    "def upgma(mat, label_list):\n",
    "    \"\"\"\n",
    "    returns a newick string representation of the tree\n",
    "    mat is the distanace matrix\n",
    "    label_list is a list of taxon names\n",
    "    \"\"\"\n",
    "    labels = label_list.copy()\n",
    "\n",
    "    while len(labels) > 1:\n",
    "        # Find index of smallest dist\n",
    "        min_i, min_j = find_min_idx(mat)\n",
    "        # Deal with the tree string, merge labels\n",
    "        labels.append(coal_label(labels[min_j],labels[min_i]))\n",
    "        # Delete orig labels\n",
    "        del labels[min_i]\n",
    "        del labels[min_j]\n",
    "        # Create a new row and column, so new clade appends to end\n",
    "        mat = np.insert(mat, mat.shape[0], values=float(0), axis=0)\n",
    "        mat = np.insert(mat, mat.shape[1], values=float(0), axis=1)\n",
    "\n",
    "        # Fill new row with average distances\n",
    "        for i in np.arange (0, mat.shape[1] - 1):\n",
    "                mat[-1][i]=mat[i][-1] = (mat[i][min_j] + mat[i][min_i])/2\n",
    "        \n",
    "        # Delete the old rows / columns we don't need        \n",
    "        mat = np.delete(mat, min_j, 0)\n",
    "        mat = np.delete(mat, min_j, 1)\n",
    "        mat = np.delete(mat, min_i-1, 0)\n",
    "        mat = np.delete(mat, min_i-1, 1)\n",
    "        \n",
    "    return(labels[0])\n",
    "\n",
    "\n",
    "labels = [\"dog\",\"cat\",\"rat\",\"mouse\",\"cow\"]\n",
    "\n",
    "aMat = np.array([[0, 17, 21, 31, 23],[17,0,30,34,21],\n",
    "                [21,30,0,28,39],[31,34,28,0,43],\n",
    "                [23,21,39,43,0]],dtype='float64')\n",
    "print(\"distance matrix:\")\n",
    "print(aMat)\n",
    "print(\"------\")\n",
    "upgma(aMat, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NJ_q_matrix(mat):\n",
    "    \"\"\"\n",
    "    Returns Q matrix calculated from distance matrix\n",
    "    according to Neighbor-Joining algorithm of\n",
    "    Saitou and Nei \n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    q = np.zeros(mat.shape)\n",
    "    sums = np.sum(mat, axis=1)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                q[i,j] = (n - 2) * mat[i,j] - sums[i] - sums [j]\n",
    "    return(q)\n",
    "  \n",
    "def NJ(mat, label_list):\n",
    "    \"\"\"\n",
    "    returns a newick string representation of the tree\n",
    "    mat is the distanace matrix\n",
    "    label_list is a list of taxon names\n",
    "    \"\"\"\n",
    "    labels = label_list.copy()\n",
    "\n",
    "    while len(labels) > 1:\n",
    "        # Calc Q matrix of net distances\n",
    "        q = NJ_q_matrix(mat)\n",
    "        # Find index of smallest dist\n",
    "        min_i, min_j = find_min_idx(q)\n",
    "        # Deal with the tree string, merge labels\n",
    "        labels.append(coal_label(labels[min_j],labels[min_i]))\n",
    "        # Delete orig labels\n",
    "        del labels[min_i]\n",
    "        del labels[min_j]\n",
    "        # Create a new row and column, so new clade appends to end\n",
    "        mat = np.insert(mat, mat.shape[0], values=float(0), axis=0)\n",
    "        mat = np.insert(mat, mat.shape[1], values=float(0), axis=1)\n",
    "\n",
    "        # Fill new row with new distances\n",
    "        for i in np.arange (0, mat.shape[1] - 1):\n",
    "                mat[-1][i]= mat[i][-1] = (mat[i][min_j] + mat[i][min_i] - mat[min_j][min_i])/2\n",
    "        \n",
    "        # Delete the old rows / columns we don't need        \n",
    "        mat = np.delete(mat, min_j, 0)\n",
    "        mat = np.delete(mat, min_j, 1)\n",
    "        mat = np.delete(mat, min_i-1, 0)\n",
    "        mat = np.delete(mat, min_i-1, 1)\n",
    "        \n",
    "    return(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "aMat = np.array([[0, 5, 9, 9, 8],[5,0,10,10,9],\n",
    "                [9,10,0,8,7],[9,10,8,0,3],\n",
    "                [8,9,7,3,0]],dtype='float64')\n",
    "nj_tree = NJ(aMat, labels)\n",
    "print(nj_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biopython\n",
    "bipython gives us lots of basic functionality for dealing with trees through the `Bio.Phylo` module. The full documentation for this module can be found [here](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc200).Functionality includes reading/writing/converting trees, tree visualization tools, and interfaces to external phylogenetics packages such as phylip and PAML\n",
    "\n",
    "\n",
    "\n",
    "We will start by reading in the tree and using the native print() to summarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Phylo\n",
    "\n",
    "tree = Phylo.read(\"data/tree.newick\", \"newick\")\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will not that a `Tree` object is returned that represents the hierarchical relationships in `Clade` objects. We won't deal with those directly today, but know that they are their and something you can manipulate. \n",
    "\n",
    "Next let's draw the tree. This is quite easy to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phylo.draw(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course do things like add or remove labels and add colors to different parts of the tree. Not that I able to find particular clades by asking for the common ancestor of two taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrca = tree.common_ancestor({\"name\": \"1\"}, {\"name\": \"3\"})\n",
    "mrca.color=\"cyan\"\n",
    "Phylo.draw(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can even draw our NJ tree from above using a quick call to the system to write the string out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "tree = Phylo.read(StringIO(NJ(aMat, labels)), \"newick\")\n",
    "Phylo.draw(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making trees from actual sequence data\n",
    "biopython has decent algorithms for creating many sorts of trees from sequence data including the NJ and UPGMA trees that we have looked at above. It does this through it's so-called constructor methods. We will play with a simple example first\n",
    "\n",
    "## a distance tree of lysozyme\n",
    "A classic molecular phylogenetics dataset that has been circulating for years is the primate lysozyme alignment. This is a simple coding sequence alignment of a single enzyme from a bunch of primates. I've included it for you here `data/lysozymeSmall.fasta`.\n",
    "\n",
    "Let's open then alignment using biopython and create and display a distance tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio.Phylo.TreeConstruction import DistanceCalculator\n",
    "from Bio import AlignIO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "lyso = AlignIO.read(\"data/lysozymeSmall.fasta\",\"fasta\")\n",
    "fig,ax = plt.subplots(figsize=[12,8])\n",
    "\n",
    "calculator = DistanceCalculator('identity')\n",
    "constructor = DistanceTreeConstructor(calculator, 'upgma')\n",
    "tree = constructor.build_tree(lyso)\n",
    "#hate the internal name labels\n",
    "for n in tree.get_nonterminals():\n",
    "    n.name=\"\"\n",
    "\n",
    "Phylo.draw(tree,axes=ax)\n",
    "fig.axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty neat huh? we can of course also get out the distance matrix that was calculated easily enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = calculator.get_distance(lyso)\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance trees for our spike protein alignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've gone ahead and created a multiple alignment file of Spike gene alignments from the `covid_small.fasta` file. This new file can be found at `data/spike_align.fasta`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Phylo.TreeConstruction import DistanceCalculator\n",
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "spike_align = AlignIO.read(\"data/spike_align.fasta\",\"fasta\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[24,18])\n",
    "constructor = DistanceTreeConstructor(calculator, 'upgma')\n",
    "tree = constructor.build_tree(spike_align)\n",
    "Phylo.draw(tree,axes=ax)\n",
    "fig.axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "The Maximum Likelihood principle says that the most probable explanation for some dataset, given a model, is the best explanation. While this might strike you as a trivially obvious point of view, mathematically it is not necessarily so. Statistical estimation using Maximum Likelihood (ML) was popularized by R. A. Fisher (of population genetics fame) in the early part of the 20th Century. ML estimation has been broadly applied across the sciences as a technology for learning about natural processes, but for our purposes we will particularly highlight its utility in estimating phylogenetic trees. \n",
    "\n",
    "The basic setup is that I want to write down a function that describes the probability of my data given the parameters of my model-- what is known in the business as the **Likelihood Function**. Abstractly then, given some dataset $X$ and some set of parameters associated with my model, call them $\\theta$, we seek to write down the likelihood of $X$ given $\\theta$ or\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta) = Prob(X | \\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "## Flipping Coins\n",
    "ML estimation is totally general, so consider some dataset $X$ that consists of multiple, independent observations from some process. $X$ could for instance be the number of times you flipped heads in a set of n coin flips. The process here, coin flipping, is associated with a parameter, the probability of getting heads. Random variables that follow this sort of process (a heads or tails outcome repeated many times) are called binomial random variables and their probabilities come from the so-called Binomial Distribution. So without further ado, the probability that I flip a coin $n$ times and end up with $X=k$ heads is\n",
    "\n",
    "\\begin{equation*}\n",
    "P(X=k| n, p) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "where $p$ is the probability that any one coin flip will come up heads. Let's write the code to calculate such probabilities, it's pretty straightforward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a slicker, and more general way to have done this would have been to try to find the maximum of this function using some mathematics. In particular there are a huge number of ways to optimize functions to find there minimum or maximum value, and many of these are already available to us through the python tools I've been introducing in class. We can dip in to the `scipy` library for what we need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# define our probability function in this case\n",
    "# the binomial probabilty mass function\n",
    "def binomial_prob(heads, n, p):\n",
    "    \"\"\"\n",
    "    returns probability of heads # of successes\n",
    "    in n flips of a coin with probability(heads) = p\n",
    "    \"\"\"\n",
    "    b_coef = np.math.factorial(n) / (np.math.factorial(n-heads) *\\\n",
    "                                     np.math.factorial(heads))\n",
    "    return(b_coef * p**heads * (1-p)**(n-heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's use this code to plot what the probability of different numbers of heads would be if I flipped a _fair_ ($p=0.5$) coin $n=10$ times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [binomial_prob(x,10,0.5) for x in range(11)]\n",
    "plt.bar(list(range(11)),probs)\n",
    "plt.xlabel(\"number of heads\")\n",
    "plt.ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "playing with this sort of function is a great way to get intuition into a statistical problem. \n",
    "\n",
    "\n",
    "**Exercise:** change the value of $p$ and rerun the code above. Does it make sense to you how the distribution is changing shape? Ask youself, what does this mean about data that I would observe.\n",
    "\n",
    "### Estimation\n",
    "Next let's turn our attention to estimation of the parameters of our model. In this case our model has a single parameter-- $p$ the probability of heads. The rest of the model is fixed-- $n$ is the number of flips i.e., the experimental design, and $k$ is my data, my actual observation. \n",
    "\n",
    "In the Maximum Likelihood world I will seek the value of my parameters that maximize the probability of my data. So in that case I want to find the value of $p$ that leads to the maximum value of the function above, given my data. Let's assume you've flipped the coin a bit more and now $n=240$ flips and let's say you've seen $k=34$ heads. Let's seek the ML estimate of $p$.\n",
    "\n",
    "One way to do this would be to plug in multiple values of the parameter to our likelihood function and compare probabilities. This is rather brute force but let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 34\n",
    "n = 240\n",
    "\n",
    "p_guesses = np.linspace(0,1,100)\n",
    "lik_p = np.zeros(len(p_guesses))\n",
    "for i,p in enumerate(p_guesses):\n",
    "    lik_p[i] = binomial_prob(data, n, p)\n",
    "\n",
    "plt.plot(p_guesses, lik_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "\n",
    "# set up a quick new function based on our old one\n",
    "# the -1 here is because scipy \n",
    "# does minimization not max\n",
    "# data is same as before\n",
    "\n",
    "def min_binomial_prob(p, data=34, n=240):\n",
    "    return(-1 * binomial_prob(data, n, p))\n",
    "\n",
    "res = optimize.minimize_scalar(min_binomial_prob, bounds=[0,1], method='bounded')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this little bit of code above is running _optimization_ for us. it is automatically finding the value of $p$ that leads to the highest likelihood of our dataset.\n",
    "\n",
    "i'm going to package this up into a small function below for you to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def estimate_binomial_p(data, n):\n",
    "    \"\"\"\n",
    "    estimates p from binomial\n",
    "    \n",
    "    counts data the hard way\n",
    "    \"\"\"\n",
    "    if data > n:\n",
    "        print(\"error, counts > n\\n\")\n",
    "        return(np.nan)\n",
    "    def min_binomial_prob(p, data=data, n=n):\n",
    "        return(-1 * binomial_prob(data, n, p))\n",
    "    \n",
    "    res = optimize.minimize_scalar(min_binomial_prob, bounds=[0,1], method='bounded')\n",
    "    return(res.x)\n",
    "\n",
    "estimate_binomial_p(9, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** play with the `estimate_binomial_p()` function above by trying out different input numbers. Do the results square with your intuition into the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxmimum Likelihood Phylogenetics\n",
    "Now that we've covered a bit of ML estimation, let's try applying it to our problem-- tree estimation. Again, stated in the business of statistics, our goal is to maximize the probability of our data (i.e. our sequence alignment) give our model (our tree and mutational model).\n",
    "\n",
    "Rather than do these calculations ourselves we will rely on an external package to calculate tree likelihoods and search for the best possible tree (a very hard problem). The package we will use is called `RAxML` and it's [documentation can be found here](https://cme.h-its.org/exelixis/web/software/raxml/index.html). \n",
    "\n",
    "Using this software from within python is a snap. As before we will use biopython for dealing with sequence data and then we will define a RAxML command to run. I'm going to do this on a newer, geographically diverse set of SARS-CoV-2 genome sequences that I have aligned for us in a new file called `data/sc_2_subset.align.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#define the command line to run\n",
    "raxml_cline = \"raxmlHPC -m GTRCAT -n out -p 10000 --silent --JC69 -s data/sc2_subset.align.fasta > /dev/null\"\n",
    "print(\"command line: \"+raxml_cline)\n",
    "\n",
    "#a function to get likelihood results\n",
    "def get_lik(file):\n",
    "    # get likelihood\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            tok = line.split()\n",
    "            if len(tok) > 0 and tok[0] == 'Final':\n",
    "                lik = float(tok[-1])\n",
    "    return(lik)\n",
    "\n",
    "# clean out previous results\n",
    "if os.path.exists(\"RAxML_info.out\"):\n",
    "    [os.remove(x) for x in os.listdir(\".\") if 'RAxML' in x]    \n",
    "\n",
    "# now run raxml using os.system\n",
    "os.system(raxml_cline)\n",
    "best_tree = Phylo.read(\"RAxML_bestTree.out\", \"newick\")\n",
    "lik = get_lik(\"RAxML_info.out\")\n",
    "print(lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's draw that best tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=[24,18])\n",
    "Phylo.draw(best_tree,axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a bit ugly, so we will probably need to do some tinkering if we want that tree to look nice. but let's focus on likelihoods for minute.\n",
    "\n",
    "## Comparing likelihoods\n",
    "One super cool thing about likelihood estimation is we can compare model fits directly by examining the likelihood of our data. let's now compare some different mutational models and look at their likelihoods. We will iterate through these models in order of increasing complexity. Recall this figure from lecture\n",
    "<img src=imgs/mutation_models.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\"JC69\", \"K80\", \"HKY85\"]:\n",
    "    raxml_cline = f\"raxmlHPC -m GTRCAT -n out -p 10000 --{m} --silent -s data/sc2_subset.align.fasta > /dev/null\"\n",
    "    print(\"command line: \"+raxml_cline)\n",
    "    # clean out previous results\n",
    "    if os.path.exists(\"RAxML_bestTree.out\"):\n",
    "        [os.remove(x) for x in os.listdir(\".\") if 'RAxML' in x]    \n",
    "\n",
    "    # now run raxml using os.system\n",
    "    os.system(raxml_cline)\n",
    "    lik = get_lik(\"RAxML_info.out\")\n",
    "    print(f\"likelihood of model {m}: {lik}\")\n",
    "    \n",
    "#now try GTR model\n",
    "raxml_cline = f\"raxmlHPC -m GTRCAT -n out -p 10000 --silent -s data/sc2_subset.align.fasta > /dev/null\"\n",
    "print(\"command line: \"+raxml_cline)\n",
    "# clean out previous results\n",
    "if os.path.exists(\"RAxML_bestTree.out\"):\n",
    "    [os.remove(x) for x in os.listdir(\".\") if 'RAxML' in x]    \n",
    "\n",
    "# now run raxml using os.system\n",
    "os.system(raxml_cline)\n",
    "lik = get_lik(\"RAxML_info.out\")\n",
    "print(f\"likelihood of model GTR: {lik}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the GTR model has the highest likelihood of these comparisons, but does it fit significantly better than say the HKY85 model? Well to compare these we might consider the number of parameters-- models with more parameters are always able to fit data better because there are more knobs to tweak-- what we call degrees of freedom in the business. \n",
    "\n",
    "ML estimates can be compared using a _likelihood ratio_. That is the ratio of the likelihoods of the two models, call them $L_1(\\theta)$ and $L_2(\\theta)$, can be used to create a test statistic such that\n",
    "\\begin{equation*}\n",
    "\\lambda = -2 \\ln \\frac{L_1(\\theta)}{L_2(\\theta)}\n",
    "\\end{equation*}\n",
    "\n",
    "where \\lambda follows a $\\chi^2$ distribution with number of degrees of freedom equal to the difference in free parameters between models. If we were to compare the GTR and the HKY85 model we would get\n",
    "\\begin{equation*}\n",
    "\\lambda = -2[-42112.246428 - -42103.229914 ] = 18.033\n",
    "\\end{equation*}\n",
    "\n",
    "in this case the $\\chi^2$ distribution with 4 degrees of freedom has a critical value of ~11 at the upper 2.5% tail, so this difference is significant and we say that the GTR model fits better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PAML for phylogenetics\n",
    "For a number of you in here phylogenetics will be one of your common use points for computational biology. biopython has all these neat built ins that help you do things efficiently, without having to reinvent the wheel. \n",
    "\n",
    "Among these there exists a nice interface with the popular PAML software package. PAML allows us to perform maximum likelihood phylogenetic estimation in a flexible way. The reason it is so popular though is that it implements a number of tests for positive selection using comparisons of the rate of substitution between synonymous sites ($dS$) and nonsynonymous sites ($dN$). Briefly PAML can estimate the ratio of substitutions rates $\\omega = dN / dS$. If all mutations had no effect on fitness than we should expect these rates to be equivalent and $\\omega = 1$. If however most mutations at nonsynonymous sites were harmful and thus deleterious to the fitness of the organism, we would expect $\\omega < 1$. If on the other nonsynonymous mutations were beneficial on average we could expect $\\omega > 1$.\n",
    "\n",
    "Let's use PAML quickly to play with these ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Phylo.PAML import codeml\n",
    "cml = codeml.Codeml(alignment = \"data/lysozymeSmall.txt\", tree = \"data/lysozymeSmall.trees\",\n",
    "                    out_file = \"results.out\", working_dir = \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set some PAML options\n",
    "#can set them manually\n",
    "cml.set_options(noisy=9,verbose=1,runmode=0,seqtype=1,model=0)\n",
    "#or with a control file\n",
    "#cml.read_ctl_file(\"data/lysozymeSmall.ctl\")\n",
    "cml.print_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great that's our run options. we can then run it simply using the biopython interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict=cml.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our data will comeback as a dictionary that we can look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in result_dict.keys():\n",
    "    print(k)\n",
    "print()    \n",
    "print(result_dict['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are the slots in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(result_dict['NSsites'][0].keys()))\n",
    "#print(list(result_dict['NSsites'][0]['parameters']['omega']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing models using PAML\n",
    "Let's compare so called Model 0, which has one dN/dS ratio along the tree, with Model 1 which allows for different rates along each branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Phylo\n",
    "\n",
    "for i in range(2):\n",
    "    cml.set_options(noisy=9,verbose=1,runmode=0,seqtype=1,model=i)\n",
    "    result_dict=cml.run()\n",
    "    print(f\"model {i} -- {result_dict['model']} lnL: {result_dict['NSsites'][0]['lnL']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy to get labelled trees from PAML output file called mlc\n",
    "\n",
    "fh = open(\"mlc\",\"r\")\n",
    "lines = fh.readlines()\n",
    "omega_tree = lines[-4].replace(\" #\",\":\")\n",
    "\n",
    "# we can plot the tree\n",
    "from io import StringIO\n",
    "tree = Phylo.read(StringIO(omega_tree), \"newick\")\n",
    "  \n",
    "for c in tree.find_clades():\n",
    "    if c.branch_length != None and  c.branch_length > 1:\n",
    "        c.color = 'red'\n",
    "  \n",
    "Phylo.draw(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
